{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# import openpyxl\n",
    "from itertools import product\n",
    "# import xlrd\n",
    "# from xlutils.copy import copy\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./csv_data/INS-W_1/FeatureData/sleep.csv')\n",
    "# df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "# df.set_index(['pid','date'])\n",
    "# data = df.drop(['pid','date'], axis=1)\n",
    "# data.select_dtypes(include=['float64']).to_numpy()\n",
    "# mask = 1 - data.isnull().astype(float)\n",
    "# # sns.heatmap(data.isnull(), cbar=False, cmap='viridis')\n",
    "# # data_normalized = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "###set parameters###\n",
    "Dim = 676\n",
    "Train_No = 14260\n",
    "mb_size = 200\n",
    "p_hint = 0.1\n",
    "gamma = 5\n",
    "alpha = 10\n",
    "beta = 0.5\n",
    "interations= 3001\n",
    "n_hidden1 = 28\n",
    "n_hidden2 = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## non functional weird shit\n",
    "data = pd.read_csv('./csv_data_/INS-W_1/FeatureData/sleep.csv')\n",
    "data = data.to_numpy()\n",
    "\n",
    "# import data with missing value - NOT SURE YET\n",
    "xx = pd.read_excel('C:/Users/Researcher/Desktop/GAN_newsimu/hpc_data/multi_accuracy/only missing data_k='\n",
    "                    + str(i) + 'misrate=' + str(j) + '.xlsx')\n",
    "\n",
    "xx = xx.as_matrix()\n",
    "\n",
    "# set up mask\n",
    "mask = np.isnan(xx)\n",
    "mask = mask + 0\n",
    "mask = 1. - mask\n",
    "\n",
    "\n",
    "# about continuous and categorical\n",
    "con = np.array(\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "con = [con, ] * Train_No\n",
    "con = np.asarray(con)\n",
    "\n",
    "\n",
    "# normalization (mydata is to use)\n",
    "xmax = np.nanmax(data, axis=0)\n",
    "\n",
    "xmin = np.nanmin(data, axis=0)\n",
    "\n",
    "max_min = xmax - xmin\n",
    "max_min = max_min + 1e-8\n",
    "mydata = ((data - xmin) / max_min) * con + (1 - con) * data\n",
    "mydata = np.asarray(mydata)\n",
    "\n",
    "# imputation value(Z)\n",
    "meanvalue = np.nanmean(a=mydata, axis=0)\n",
    "meanZ = [meanvalue, ] * Train_No\n",
    "meanZ = np.asarray(meanZ)\n",
    "\n",
    "##define write to excel in for circle\n",
    "'''\n",
    "def write_excel_xls_append(path, value):\n",
    "    index = len(value)  # get the No of lines of the data to be write\n",
    "    workbook = xlrd.open_workbook(path)  # open the file\n",
    "    sheets = workbook.sheet_names()  # get all the sheets\n",
    "    worksheet = workbook.sheet_by_name(sheets[0])  # get the first sheet\n",
    "    rows_old = worksheet.nrows  # get the lines No already in the sheet\n",
    "    new_workbook = copy(workbook)  # transfer xlrd to xlwt\n",
    "    new_worksheet = new_workbook.get_sheet(0)  # get the first sheet of transfered sheet\n",
    "    for i in range(0, index):\n",
    "        for j in range(0, len(value[i])):\n",
    "            new_worksheet.write(i + rows_old, j, value[i][j])  # write from No i+rows_old line\n",
    "    new_workbook.save(path)\n",
    "    print(\"write to excel done！\")\n",
    "'''\n",
    "\n",
    "# define loss\n",
    "D_loss = -tf.reduce_mean(M * tf.math.log(tf.clip_by_value(D_result, 1e-8, 1.)) + (1 - M) * tf.math.log(\n",
    "    tf.clip_by_value(1. - D_result, 1e-8, 1.))) * 2  # D_loss is sigmoid cross-entropy to tell true/fake\n",
    "\n",
    "MSE_train = tf.reduce_mean((M * X * CO - M * G_sample * CO) ** 2) / tf.reduce_mean(M * CO)\n",
    "CROSS_train = -tf.reduce_mean(\n",
    "    (1 - CO) * X * M * tf.math.log(\n",
    "        tf.clip_by_value(\n",
    "            G_sample, 1e-8, 1) \n",
    "            ) + (1 - X) * (1 - CO) * M * tf.math.log(\n",
    "        tf.clip_by_value(1 - G_sample, 1e-8, 1.)\n",
    "        )\n",
    "    )\n",
    "G_loss = gamma * G_loss1 + alpha * MSE_train + beta * CROSS_train\n",
    "\n",
    "\n",
    "### RUN\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import full dataset\n",
    "\n",
    "df = pd.read_csv('./csv_data/INS-W_1/FeatureData/sleep.csv')\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df.set_index(['pid','date'])\n",
    "data = df.drop(['pid','date'], axis=1)\n",
    "\n",
    "# set up mask\n",
    "data = data.select_dtypes(include=[float])\n",
    "mask = (1 - data.isnull().astype(float))\n",
    "mask = mask.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data.fillna(0, inplace=True)\n",
    "data = data.select_dtypes(include=[float]).to_numpy()\n",
    "data = data.astype(float)\n",
    "\n",
    "data_normalized = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation value(Z)\n",
    "meanvalue = np.nanmean(a=data_normalized, axis=0)\n",
    "meanZ = [meanvalue, ] * Train_No\n",
    "meanZ = np.asarray(meanZ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "###DEFINE function\n",
    "\n",
    "# Z start from noise\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(0., 1, size=[m, n])\n",
    "\n",
    "\n",
    "# for hint\n",
    "def sample_M(m, n, p):\n",
    "    A = np.random.uniform(0., 1., size=[m, n])\n",
    "    B = A > p\n",
    "    C = 1. * B\n",
    "    return C\n",
    "\n",
    "\n",
    "# for sampling mb\n",
    "def sample_idx(m, n):\n",
    "    A = np.random.permutation(m)\n",
    "    idx = A[:n]\n",
    "    return idx\n",
    "\n",
    "\n",
    "# for start value\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "# for draw a picture\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "    gs = gridspec.GridSpec(5, 5)\n",
    "\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "\n",
    "        plt.axis('off')\n",
    "\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "        plt.imshow(sample.reshape(5, 10), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "##define batch normalization\n",
    "\n",
    "def dense(x, size, scope):\n",
    "    with tf.name_scope(scope):\n",
    "        return Dense(size, activation=None)(x)\n",
    "\n",
    "def dense_batch_relu(x, size, phase, scope):\n",
    "    with tf.name_scope(scope):\n",
    "        h1 = Dense(size, activation=None)(x)\n",
    "        h2 = BatchNormalization(center=True, scale=True)(h1, training=phase)\n",
    "        return Activation('tanh')(h2)\n",
    "    \n",
    "###BUILD STRUCTURE\n",
    "# placeholder\n",
    "\n",
    "with tf.name_scope('input_layer'):\n",
    "    X = tf.keras.Input(shape=(Dim,), dtype=tf.float32, name='X_input')  # data vector with missing value\n",
    "    M = tf.keras.Input(shape=(Dim,), dtype=tf.float32, name='Y')  # mask vector\n",
    "    H = tf.keras.Input(shape=(Dim,), dtype=tf.float32, name='H')  # hint vector\n",
    "    Z = tf.keras.Input(shape=(Dim,), dtype=tf.float32, name='Z')  # mean vector\n",
    "    CO = tf.keras.Input(shape=(Dim,), dtype=tf.float32, name='CO')  # continuous variables vector\n",
    "    phase1 = tf.keras.Input(shape=(), dtype=tf.bool, name='phase1')\n",
    "    phase2 = tf.keras.Input(shape=(), dtype=tf.bool, name='phase2')\n",
    "    \n",
    "# Define generator\n",
    "\n",
    "class sigmoidLayer(keras.Layer):\n",
    "    def call(self, x):\n",
    "        return tf.nn.sigmoid(x)\n",
    "\n",
    "class convertToTensor(keras.Layer):\n",
    "    def call(self, x):\n",
    "        return tf.convert_to_tensor(x, dtype=tf.float32)    \n",
    "# def convertToTensor(arg):\n",
    "#     arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n",
    "#     return arg\n",
    "#     # def call(self, x, dtype):\n",
    "#     #     return tf.convert_to_tensor(x, dtype=dtype)\n",
    "\n",
    "class tfConcat(keras.Layer):\n",
    "    def call(self, values):\n",
    "        return tf.concat(values=values, axis=1)\n",
    "\n",
    "# def tfConcat(values):\n",
    "#     return tf.concat(values=values, axis=1)\n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, mask, generated_sample, hint):\n",
    "        # Combine inputs, mask, generated sample, and hint\n",
    "        x = tf.concat([inputs, mask, generated_sample, hint], axis=1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, noise, mask):\n",
    "        # Combine inputs, noise, and mask\n",
    "        x = tf.concat([inputs, noise, mask], axis=1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generator(x, z, m):\n",
    "    inp = x * m + z * (1 - m)\n",
    "    input = tfConcat()([inp, m])\n",
    "    \n",
    "    # Use dense_batch_relu and dense functions\n",
    "    G_h1 = dense_batch_relu(input, n_hidden1, phase1, 'Glayer1')\n",
    "    G_h2 = dense_batch_relu(G_h1, n_hidden2, phase1, 'Glayer2')\n",
    "    G_logits = tf.keras.layers.Dense(Dim, activation=None, name='G_logits')(G_h2)\n",
    "\n",
    "    G_prob = sigmoidLayer()(G_logits)\n",
    "    \n",
    "    return G_prob\n",
    "\n",
    "def discriminator(x, m, g, h):\n",
    "    inp = x * m + g * (1 - m)\n",
    "    inp = convertToTensor()(inp)  # Ensure inp is a tensor\n",
    "    h = convertToTensor()(h)      # Ensure h is a tensor\n",
    "    input = tfConcat()(values=[inp, h])\n",
    "    # D_hidden1_1 = tf.layers.dense(input, n_hidden1, name=\"D_hidden1\")\n",
    "    # D_bn1 = tf.layers.batch_normalization(D_hidden1_1, training=training2, momentum=0.9)\n",
    "    # D_hidden1 = tf.nn.elu(D_bn1)\n",
    "\n",
    "    # D_hidden2_1 = tf.layers.dense(D_hidden1, n_hidden2, name=\"D_hidden2\")\n",
    "    # D_bn2 = tf.layers.batch_normalization(D_hidden2_1, training=training2, momentum=0.9)\n",
    "    # D_hidden2 = tf.nn.elu(D_bn2)\n",
    "\n",
    "    # D_logits_before_bn = tf.layers.dense(D_hidden2, Dim, name=\"D_outputs\")\n",
    "    # D_logit = tf.layers.batch_normalization(D_logits_before_bn, training=training2,momentum=0.9)\n",
    "\n",
    "    D_h1 = dense_batch_relu(input, n_hidden1, phase2, 'Dlayer1')\n",
    "    D_h2 = dense_batch_relu(D_h1, n_hidden2, phase2, 'D_layer2')\n",
    "    D_logits = dense(x=D_h2, size=Dim, scope='D_s_logits')\n",
    "    D_prob = sigmoidLayer()(D_logits)\n",
    "    return D_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model constructure\n",
    "\n",
    "class gSampleLayer(keras.Layer):\n",
    "    def call(self, x, z ,m):\n",
    "        return generator(x, z, m)\n",
    "\n",
    "class dResultLayer(keras.Layer):\n",
    "    def call(self, x, m, g, h):\n",
    "        return discriminator(x, m, g, h)\n",
    "\n",
    "G_sample = gSampleLayer()(X, Z, M)\n",
    "D_result = dResultLayer()(X, M, G_sample, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keras tensor to tensorflow function\n",
    "\n",
    "# class reduceMean(keras.Layer):\n",
    "#     def call(self, x):\n",
    "#         return tf.reduce_mean(x)\n",
    "\n",
    "class reduceMean(tf.keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        # Debug: Check the type of the input tensor\n",
    "        print(\"Type of input to reduceMean:\", type(x))\n",
    "        print(\"Shape of input to reduceMean:\", x.shape)\n",
    "\n",
    "        # Check if the input is a SparseTensor\n",
    "        if isinstance(x, tf.SparseTensor):\n",
    "            x = tf.sparse.to_dense(x)  # Convert SparseTensor to dense\n",
    "\n",
    "        # Apply tf.reduce_mean to the dense tensor\n",
    "        return tf.reduce_mean(x)\n",
    "\n",
    "class clipByVal(keras.Layer):\n",
    "    def call(self, x, min_value, max_value):\n",
    "        return tf.clip_by_value(x, min_value, max_value)\n",
    "\n",
    "class mathLog(keras.Layer):\n",
    "    def call(self, x):\n",
    "        return tf.math.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (None, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (None, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (None, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (None, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (None, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (None, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (None, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (None, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (None, 676)\n"
     ]
    }
   ],
   "source": [
    "# define loss\n",
    "# D_loss is sigmoid cross-entropy to tell true/fake\n",
    "D_loss = reduceMean()(M * mathLog()(\n",
    "    clipByVal()(D_result, min_value=1e-8, max_value=1.)) + (1 - M) * mathLog()(\n",
    "    clipByVal()(1. - D_result, min_value=1e-8, max_value=1.))) * 2\n",
    "\n",
    "# G_loss1 to minimize the possibility that D tells true/fake\n",
    "G_loss1 = reduceMean()(\n",
    "    (1 - M) * mathLog()(\n",
    "    clipByVal()(D_result, min_value=1e-8, max_value=1.)\n",
    "    )\n",
    ") / reduceMean()(1 - M)  \n",
    "\n",
    "MSE_train = reduceMean()( (M * X * CO - M * G_sample * CO) ** 2) / reduceMean()(M * CO)\n",
    "CROSS_train = reduceMean()(\n",
    "    (1 - CO) * X * M * mathLog()(\n",
    "        clipByVal()(\n",
    "            G_sample, min_value=1e-8, max_value=1.)\n",
    "    ) + (1 - X) * (1 - CO) * M * mathLog()((\n",
    "        clipByVal()(1 - G_sample, min_value=1e-8, max_value=1.)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# MIGHT ADD LATER? IDK WHAT THIS EVEN DOES\n",
    "# with tf.name_scope('G_loss'):\n",
    "#                 G_loss = gamma * G_loss1 + alpha * MSE_train + beta * CROSS_train\n",
    "#                 tf.summary.scalar('G_loss', G_loss)\n",
    "\n",
    "# test performance\n",
    "\n",
    "MSE_test = reduceMean()(((1 - M) * X * CO - (1 - M) * G_sample * CO) ** 2) / reduceMean()((1 - M) * CO)\n",
    "CROSS_test = -reduceMean()(\n",
    "    (1 - CO) * X * (1 - M) * mathLog()(\n",
    "        clipByVal()(G_sample, min_value=1e-8, max_value=1.)) + (1 - X) * (1 - CO) * (\n",
    "        1 - M) * mathLog()(\n",
    "        clipByVal()(1 - G_sample, min_value=1e-8, max_value=1.)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# MSE_test = tf.reduce_mean(((1 - M) * X * CO - (1 - M) * G_sample * CO) ** 2) / tf.reduce_mean((1 - M) * CO)\n",
    "# CROSS_test = -tf.reduce_mean(\n",
    "#     (1 - CO) * X * (1 - M) * tf.math.log(tf.clip_by_value(G_sample, 1e-8, 1.)) + (1 - X) * (1 - CO) * (\n",
    "#         1 - M) * tf.math.log(\n",
    "#         tf.clip_by_value(1 - G_sample, 1e-8, 1.)))\n",
    "\n",
    "# optimizer\n",
    "optimizer_D = tf.keras.optimizers.SGD(learning_rate=0.2)\n",
    "optimizer_G = tf.keras.optimizers.SGD(learning_rate=0.5)\n",
    "\n",
    "# training steps\n",
    "@tf.function\n",
    "def train_step(X_mb, M_mb, Z_mb, H_mb, CO_mb):\n",
    "    with tf.GradientTape() as tape_D, tf.GradientTape() as tape_G:\n",
    "        G_sample = generator(X_mb, Z_mb, M_mb)\n",
    "        D_result = discriminator(X_mb, M_mb, G_sample, H_mb)\n",
    "        \n",
    "        D_loss = -reduceMean()(\n",
    "            M_mb * mathLog()(\n",
    "                clipByVal()(D_result, min_value=1e-8, max_value=1.)) + (1 - M_mb) * mathLog()(\n",
    "            clipByVal()(1. - D_result, min_value=1e-8, max_value=1.))) * 2\n",
    "        G_loss1 = -reduceMean()((1 - M_mb) * mathLog()(clipByVal()(D_result, min_value=1e-8, max_value=1.))) / reduceMean()(\n",
    "            1 - M_mb)\n",
    "        MSE_train = reduceMean()((M_mb * X_mb * CO_mb - M_mb * G_sample * CO_mb) ** 2) / reduceMean()(M_mb * CO_mb)\n",
    "        CROSS_train = -reduceMean()(\n",
    "            (1 - CO_mb) * X_mb * M_mb * mathLog()(clipByVal()(G_sample, min_value=1e-8, max_value=1)) + (1 - X_mb) * (1 - CO_mb) * M_mb * mathLog()(\n",
    "                clipByVal()(1 - G_sample, min_value=1e-8, max_value=1.)))\n",
    "        G_loss = gamma * G_loss1 + alpha * MSE_train + beta * CROSS_train\n",
    "\n",
    "    gradients_D = tape_D.gradient(D_loss, discriminator.trainable_variables)\n",
    "    gradients_G = tape_G.gradient(G_loss, generator.trainable_variables)\n",
    "    optimizer_D.apply_gradients(zip(gradients_D, discriminator.trainable_variables))\n",
    "    optimizer_G.apply_gradients(zip(gradients_G, generator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = np.ones_like(data_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfMultiply(keras.Layer):\n",
    "    def call(self, x, y):\n",
    "        return tf.multiply(x, y)\n",
    "    \n",
    "class tfRound(keras.Layer):\n",
    "    def call(self, x):\n",
    "        return tf.round(x)\n",
    "    \n",
    "class tfRange(keras.Layer):\n",
    "    def call(self, x, y):\n",
    "        return tf.range(x, y)\n",
    "    \n",
    "class tensor_scatter_nd_update(keras.Layer):\n",
    "    def call(self, x, indices, updates):\n",
    "        return tf.tensor_scatter_nd_update(x, indices, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (200, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (200, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (200, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (200, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (200, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (200, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (200, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (200, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (200, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (200, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (200, 676)\n",
      "Type of input to reduceMean: <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Shape of input to reduceMean: (200, 676)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/3001 [00:00<01:37, 30.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraction:0\n",
      "G_loss:<KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2750>\n",
      "D_loss:<KerasTensor shape=(), dtype=float32, sparse=True, ragged=False, name=keras_tensor_2742>\n",
      "Train_MSE_loss: <KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2760>\n",
      "Train_CROSS_loss: <KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2776>\n",
      "Test_MSE_loss: <KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2789>\n",
      "Test_CROSS_loss: <KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2808>\n",
      "Final_MSE: <KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2789>\n",
      "Final_CROSS: <KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2808>\n",
      "Type of G_loss1: <class 'keras.src.backend.common.keras_tensor.KerasTensor'>\n",
      "Type of D_loss: <class 'keras.src.backend.common.keras_tensor.KerasTensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:33<00:00, 90.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraction:3000\n",
      "G_loss:<KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2750>\n",
      "D_loss:<KerasTensor shape=(), dtype=float32, sparse=True, ragged=False, name=keras_tensor_2742>\n",
      "Train_MSE_loss: <KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2760>\n",
      "Train_CROSS_loss: <KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2776>\n",
      "Test_MSE_loss: <KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2789>\n",
      "Test_CROSS_loss: <KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2808>\n",
      "Final_MSE: <KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2789>\n",
      "Final_CROSS: <KerasTensor shape=(), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2808>\n",
      "Type of G_loss1: <class 'keras.src.backend.common.keras_tensor.KerasTensor'>\n",
      "Type of D_loss: <class 'keras.src.backend.common.keras_tensor.KerasTensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for it in tqdm(range(interations)):\n",
    "    mb_idx = sample_idx(Train_No, mb_size)\n",
    "    X_mb = data_normalized[mb_idx, :]\n",
    "    M_mb = mask[mb_idx, :]\n",
    "    Z_mb = meanZ[mb_idx, :]\n",
    "    H_mb1 = sample_M(mb_size, Dim, 1 - p_hint)\n",
    "    H_mb = M_mb * H_mb1\n",
    "    CO_mb = con[mb_idx, :]\n",
    "    startX = M_mb * X_mb + (1 - M_mb) * Z_mb\n",
    "\n",
    "    # Convert NumPy arrays to TensorFlow tensors\n",
    "    X_mb_tf = convertToTensor()(X_mb)\n",
    "    M_mb_tf = convertToTensor()(M_mb)\n",
    "    Z_mb_tf = convertToTensor()(Z_mb)\n",
    "    H_mb_tf = convertToTensor()(H_mb)\n",
    "    CO_mb_tf = convertToTensor()(CO_mb)\n",
    "\n",
    "    train_step(X_mb_tf, M_mb_tf, Z_mb_tf, H_mb_tf, CO_mb_tf)\n",
    "\n",
    "    if it % 3000 == 0:\n",
    "        # Convert symbolic tensors to NumPy arrays or floats\n",
    "        def evaluate_tensor(tensor):\n",
    "            try:\n",
    "                return tensor.numpy()  # For eager tensors\n",
    "            except AttributeError:\n",
    "                return tf.keras.backend.get_value(tensor)  # For symbolic tensors\n",
    "\n",
    "        G_loss_curr = evaluate_tensor(G_loss1)\n",
    "        D_loss_curr = evaluate_tensor(D_loss)\n",
    "        MSE_train_curr = evaluate_tensor(MSE_train)\n",
    "        CROSS_train_curr = evaluate_tensor(CROSS_train)\n",
    "        MSE_test_curr = evaluate_tensor(MSE_test)\n",
    "        CROSS_test_curr = evaluate_tensor(CROSS_test)\n",
    "\n",
    "        MSE_final = MSE_test_curr  # Reuse the already computed value\n",
    "        CROSS_final = CROSS_test_curr  # Reuse the already computed value\n",
    "\n",
    "        # Print the results\n",
    "        print('Iteraction:{}'.format(it))\n",
    "        print('G_loss:{}'.format(G_loss_curr))\n",
    "        print('D_loss:{}'.format(D_loss_curr))\n",
    "        print('Train_MSE_loss: {}'.format(MSE_train_curr))\n",
    "        print('Train_CROSS_loss: {}'.format(CROSS_train_curr))\n",
    "        print('Test_MSE_loss: {}'.format(MSE_test_curr))\n",
    "        print('Test_CROSS_loss: {}'.format(CROSS_test_curr))\n",
    "        print('Final_MSE: {}'.format(MSE_final))\n",
    "        print('Final_CROSS: {}'.format(CROSS_final))\n",
    "        print(\"Type of G_loss1:\", type(G_loss1))\n",
    "        print(\"Type of D_loss:\", type(D_loss))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  if it % 3000 == 0:\n",
    "            X_final = data_normalized\n",
    "            M_final = mask\n",
    "            Z_final = meanZ\n",
    "            CO_final = con\n",
    "            finalX = tf.multiply(M_final, X_final) + tf.multiply((1 - M_final), Z_final)\n",
    "            max_values = np.max(data, axis=0)  # Replace with your original data\n",
    "            min_values = np.min(data, axis=0)\n",
    "            max_min = max_values - min_values\n",
    "            xmin = min_values\n",
    "\n",
    "            # Generate samples using the generator\n",
    "            X_final_tf = convertToTensor(X_final)\n",
    "            Z_final_tf = convertToTensor(Z_final)\n",
    "            M_final_tf = convertToTensor(M_final)\n",
    "            sample_final1 = generator(X_final_tf, Z_final_tf, M_final_tf)\n",
    "\n",
    "            # Perform the imputation using TensorFlow operations\n",
    "            sample_final2 = tfMultiply()(X_final, M_final) + tfMultiply()((1 - M_final), sample_final1)\n",
    "            sample_final = tfMultiply()(sample_final2, max_min) + xmin\n",
    "            sample_final = tensor_scatter_nd_update()(\n",
    "                sample_final,\n",
    "                tfRange()(x=15, y=22),\n",
    "                updates=tfRound()(sample_final[:, 15:22]))\n",
    "\n",
    "            # Export the results to an Excel file\n",
    "            sample_final_np = tf.keras.backend.eval(sample_final)  # Evaluate the symbolic tensor\n",
    "            writer = pd.ExcelWriter('./csv_imputed/INS-W_1/FeatureData/sleep_imputed_' + str(it) + '.xlsx')\n",
    "            sample_final_write = pd.DataFrame(sample_final_np)\n",
    "            sample_final_write.to_excel(writer, 'Sheet1')\n",
    "            writer.save()\n",
    "            print(f\"Imputed table exported to './csv_imputed/INS-W_1/FeatureData/sleep_imputed_{it}.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# solver\n",
    "# In TensorFlow 2.x, updates are automatically applied during the training step.\n",
    "# You do not need to explicitly manage update operations like in TensorFlow 1.x.\n",
    "# The optimizers are already defined as `optimizer_D` and `optimizer_G`.\n",
    "# The training logic is handled in the `train_step` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN\n",
    "# TensorFlow 2.x does not require sessions or manual initialization\n",
    "# If you need to log summaries, use the TensorFlow 2.x summary API\n",
    "log_dir = \"logs/\"\n",
    "writer = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3001 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'con' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[231]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m H_mb1 = sample_M(mb_size, Dim, \u001b[32m1\u001b[39m - p_hint)\n\u001b[32m     34\u001b[39m H_mb = M_mb * H_mb1\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m CO_mb = \u001b[43mcon\u001b[49m[mb_idx, :]\n\u001b[32m     36\u001b[39m startX = M_mb * X_mb + (\u001b[32m1\u001b[39m - M_mb) * Z_mb\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m##final\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'con' is not defined"
     ]
    }
   ],
   "source": [
    "# solver\n",
    "optimizer_D = tf.keras.optimizers.SGD(learning_rate=0.2)\n",
    "optimizer_G = tf.keras.optimizers.SGD(learning_rate=0.5)\n",
    "\n",
    "@tf.function\n",
    "def train_step(X_mb, M_mb, Z_mb, H_mb, CO_mb):\n",
    "    with tf.GradientTape() as tape_D, tf.GradientTape() as tape_G:\n",
    "        G_sample = generator(X_mb, Z_mb, M_mb)\n",
    "        D_result = discriminator(X_mb, M_mb, G_sample, H_mb)\n",
    "        \n",
    "        D_loss = -tf.reduce_mean(M_mb * tf.math.log(tf.clip_by_value(D_result, 1e-8, 1.)) + (1 - M_mb) * tf.math.log(\n",
    "            tf.clip_by_value(1. - D_result, 1e-8, 1.))) * 2\n",
    "        G_loss1 = -tf.reduce_mean((1 - M_mb) * tf.math.log(tf.clip_by_value(D_result, 1e-8, 1.))) / tf.reduce_mean(\n",
    "            1 - M_mb)\n",
    "        MSE_train = tf.reduce_mean((M_mb * X_mb * CO_mb - M_mb * G_sample * CO_mb) ** 2) / tf.reduce_mean(M_mb * CO_mb)\n",
    "        CROSS_train = -tf.reduce_mean(\n",
    "            (1 - CO_mb) * X_mb * M_mb * tf.math.log(tf.clip_by_value(G_sample, 1e-8, 1)) + (1 - X_mb) * (1 - CO_mb) * M_mb * tf.math.log(\n",
    "                tf.clip_by_value(1 - G_sample, 1e-8, 1.)))\n",
    "        G_loss = gamma * G_loss1 + alpha * MSE_train + beta * CROSS_train\n",
    "\n",
    "    gradients_D = tape_D.gradient(D_loss, discriminator.trainable_variables)\n",
    "    gradients_G = tape_G.gradient(G_loss, generator.trainable_variables)\n",
    "    optimizer_D.apply_gradients(zip(gradients_D, discriminator.trainable_variables))\n",
    "    optimizer_G.apply_gradients(zip(gradients_G, generator.trainable_variables))\n",
    "\n",
    "###for calculation moving_mean and moving_variance to calculate BN\n",
    "for it in tqdm(range(interations)):\n",
    "    mb_idx = sample_idx(Train_No, mb_size)\n",
    "    X_mb = data_normalized[mb_idx, :]\n",
    "    M_mb = mask[mb_idx, :]\n",
    "    Z_mb = meanZ[mb_idx, :]\n",
    "    # Z_mb = sample_Z(mb_size, Dim)\n",
    "    H_mb1 = sample_M(mb_size, Dim, 1 - p_hint)\n",
    "    H_mb = M_mb * H_mb1\n",
    "    CO_mb = con[mb_idx, :]\n",
    "    startX = M_mb * X_mb + (1 - M_mb) * Z_mb\n",
    "\n",
    "    ##final\n",
    "    finalX = (mask * data_normalized) + (1 - mask) * meanZ\n",
    "    H_final1 = sample_M(Train_No, Dim, 1 - p_hint)\n",
    "    H_final = H_final1 * mask\n",
    "    ###\n",
    "    train_step(X_mb, M_mb, startX, H_mb, CO_mb)\n",
    "    D_loss_curr = D_loss.numpy()\n",
    "    G_loss_curr = G_loss1.numpy()\n",
    "    MSE_train_curr = MSE_train.numpy()\n",
    "    CROSS_train_curr = CROSS_train.numpy()\n",
    "    MSE_test_curr = MSE_test.numpy()\n",
    "    CROSS_test_curr = CROSS_test.numpy()\n",
    "    MSE_final = MSE_test.numpy()\n",
    "    CROSS_final = CROSS_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3001 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (14260,676) (5000,676) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[230]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m startX = M_mb * X_mb + (\u001b[32m1\u001b[39m - M_mb) * Z_mb\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m##final\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m finalX = (mask * data_normalized) + \u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeanZ\u001b[49m\n\u001b[32m     15\u001b[39m H_final1 = sample_M(Train_No, Dim, \u001b[32m1\u001b[39m - p_hint)\n\u001b[32m     16\u001b[39m H_final = H_final1 * mask\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (14260,676) (5000,676) "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3001 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (14260,676) (5000,676) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[227]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m startX = M_mb * X_mb + (\u001b[32m1\u001b[39m - M_mb) * Z_mb\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m##final\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m finalX = mask * data_normalized + \u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeanZ\u001b[49m\n\u001b[32m     40\u001b[39m H_final1 = sample_M(Train_No, Dim, \u001b[32m1\u001b[39m - p_hint)\n\u001b[32m     41\u001b[39m H_final = H_final1 * mask\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (14260,676) (5000,676) "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if it % 3000 == 0:\n",
    "    print('Iteraction:{}'.format(it))\n",
    "    print('G_loss:{:.4}'.format(G_loss_curr))\n",
    "    print('D_loss:{:.4}'.format(D_loss_curr))\n",
    "    print('Train_MSE_loss: {:.4}'.format(MSE_train_curr))\n",
    "    print('Train_CROSS_loss: {:.4}'.format(CROSS_train_curr))\n",
    "\n",
    "    print('Test_MSE_loss: {:.4}'.format(MSE_test_curr))\n",
    "    print('Test_CROSS_loss: {:.4}'.format(CROSS_test_curr))\n",
    "\n",
    "    print('Final_MSE: {:.4}'.format(MSE_final))\n",
    "    print('Final_CROSS: {:.4}'.format(CROSS_final))\n",
    "    print()\n",
    "\n",
    "    # it_No = it\n",
    "    # output = np.array([[it_No, G_loss_curr, D_loss_curr,\n",
    "    # MSE_train_curr, CROSS_train_curr, MSE_test_curr, CROSS_test_curr, MSE_final, CROSS_final], ])\n",
    "    # excel_name = 'Hyper_para_search_new_March13.xls'\n",
    "    # write_excel_xls_append(excel_name, output)\n",
    "\n",
    "if it % 3000 == 0:\n",
    "    X_final = mydata\n",
    "    M_final = mask\n",
    "    Z_final = meanZ\n",
    "    CO_final = con\n",
    "    finalX = M_final * X_final + (1 - M_final) * Z_final\n",
    "    sample_final1 = sess.run(G_sample,\n",
    "                                feed_dict={X: mydata, M: mask, Z: finalX, CO: con, phase1: 0, phase2: 0})\n",
    "    sample_final2 = X_final * M_final + (1 - M_final) * sample_final1\n",
    "    sample_final = sample_final2 * max_min + xmin\n",
    "    sample_final[:, 15:22] = np.round(sample_final[:, 15:22], 0)\n",
    "    ##export\n",
    "    writer = pd.ExcelWriter('imputed_data/INS-W_1/FeatureData/sleep_imputed.xlsx')\n",
    "    sample_final_write = pd.DataFrame(sample_final)\n",
    "    # sample_final_write = sample_final_write.round({'15':0,'16':0,'17':0,'18':0,'19':0,'20':0})\n",
    "    sample_final_write.to_excel(writer, 'Sheet1')\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MSE_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[165]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFinal_MSE: \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[33m'\u001b[39m.format(\u001b[43mMSE_final\u001b[49m))\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFinal_CROSS: \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[33m'\u001b[39m.format(CROSS_final))\n",
      "\u001b[31mNameError\u001b[39m: name 'MSE_final' is not defined"
     ]
    }
   ],
   "source": [
    "print('Final_MSE: {:.4}'.format(MSE_final))\n",
    "print('Final_CROSS: {:.4}'.format(CROSS_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parameters to be calculate by the GAIN(theta_D, theta_G)\n",
    "'''\n",
    "D_W1 = tf.Variable(xavier_init([Dim * 2, n_hidden1]))  # Data and Hint are inputs\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[n_hidden1]))\n",
    "D_W2 = tf.Variable(xavier_init([n_hidden1, n_hidden2]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[n_hidden2]))\n",
    "D_W3 = tf.Variable(xavier_init([n_hidden2, Dim]))\n",
    "D_b3 = tf.Variable(tf.zeros(shape=[Dim]))\n",
    "theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([Dim * 2, n_hidden1]))  # Data and hint as inputs (mean value are in Missing Components)\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[n_hidden1]))\n",
    "G_W2 = tf.Variable(xavier_init([n_hidden1, n_hidden2]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[n_hidden2]))\n",
    "G_W3 = tf.Variable(xavier_init([n_hidden2, Dim]))\n",
    "G_b3 = tf.Variable(tf.zeros(shape=[Dim]))\n",
    "theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
    "'''\n",
    "###for calculation moving_mean and moving_variance to calculate BN\n",
    "'''\n",
    "if not os.path.exists('Dovey_exp_out1/'):\n",
    "    os.makedirs('Dovey_exp_out1/')\n",
    "\n",
    "i = 1\n",
    "'''\n",
    "\n",
    "    '''if it % 500 == 0:\n",
    "        mb_idx = sample_idx(Train_No, 5)\n",
    "        X_mb = mydata[mb_idx, :]\n",
    "        M_mb = mask[mb_idx, :]\n",
    "        Z_mb = meanZ[mb_idx, :]\n",
    "        startX = M_mb * X_mb + (1 - M_mb) * Z_mb\n",
    "        samples1 = X_mb\n",
    "        samples2 = M_mb * X_mb + (1 - M_mb) * Z_mb\n",
    "        samples31 = sess.run(G_sample, feed_dict={X: X_mb, M: M_mb, Z: startX})\n",
    "        samples3 = M_mb * X_mb + (1 - M_mb) * samples31\n",
    "        samples = np.vstack([samples1, samples2, samples3])\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('Dovey_exp_out1/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)\n",
    "    '''\n",
    "    if it % 3000 == 0:\n",
    "        print('Iteraction:{}'.format(it))\n",
    "        print('G_loss:{:.4}'.format(G_loss_curr))\n",
    "        print('D_loss:{:.4}'.format(D_loss_curr))\n",
    "        print('Train_MSE_loss: {:.4}'.format(MSE_train_curr))\n",
    "        print('Train_CROSS_loss: {:.4}'.format(CROSS_train_curr))\n",
    "\n",
    "        print('Test_MSE_loss: {:.4}'.format(MSE_test_curr))\n",
    "        print('Test_CROSS_loss: {:.4}'.format(CROSS_test_curr))\n",
    "\n",
    "        print('Final_MSE: {:.4}'.format(MSE_final))\n",
    "        print('Final_CROSS: {:.4}'.format(CROSS_final))\n",
    "        print()\n",
    "\n",
    "        # it_No = it\n",
    "        # output = np.array([[it_No, G_loss_curr, D_loss_curr,\n",
    "        # MSE_train_curr, CROSS_train_curr, MSE_test_curr, CROSS_test_curr, MSE_final, CROSS_final], ])\n",
    "        # excel_name = 'Hyper_para_search_new_March13.xls'\n",
    "        # write_excel_xls_append(excel_name, output)\n",
    "\n",
    "    if it % 3000 == 0:\n",
    "        X_final = mydata\n",
    "        M_final = mask\n",
    "        Z_final = meanZ\n",
    "        CO_final = con\n",
    "        finalX = M_final * X_final + (1 - M_final) * Z_final\n",
    "        sample_final1 = sess.run(G_sample,\n",
    "                                    feed_dict={X: mydata, M: mask, Z: finalX, CO: con, phase1: 0, phase2: 0})\n",
    "        sample_final2 = X_final * M_final + (1 - M_final) * sample_final1\n",
    "        sample_final = sample_final2 * max_min + xmin\n",
    "        sample_final[:, 15:22] = np.round(sample_final[:, 15:22], 0)\n",
    "        ##export\n",
    "        writer = pd.ExcelWriter('C:/Users/Researcher/Desktop/GAN_newsimu/hpc_data/multi_accuracy/gain_k='\n",
    "                                + str(i) + 'misrate=' + str(j) + '_i='+str(q)+'.xlsx')\n",
    "        sample_final_write = pd.DataFrame(sample_final)\n",
    "        # sample_final_write = sample_final_write.round({'15':0,'16':0,'17':0,'18':0,'19':0,'20':0})\n",
    "        sample_final_write.to_excel(writer, 'Sheet1')\n",
    "        writer.save()\n",
    "    '''\n",
    "    if it % 15000 ==0:\n",
    "        it_No = it\n",
    "        output = np.array([[mb_size,p_hint,gamma, alpha,beta, it_No,G_loss_curr, D_loss_curr,MSE_train_curr,\n",
    "                            CROSS_train_curr, MSE_test_curr,CROSS_test_curr, MSE_final, CROSS_final],])\n",
    "        excel_name = 'Hyper_para_search_weights.xls'\n",
    "        write_excel_xls_append(excel_name, output)\n",
    "    '''\n",
    "print('Final_MSE: {:.4}'.format(MSE_final))\n",
    "print('Final_CROSS: {:.4}'.format(CROSS_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in (0.4,0.5):\n",
    "    for i in (1,2,3):\n",
    "        for q in range(1,31):\n",
    "            # import full dataset\n",
    "            tf.reset_default_graph()\n",
    "\n",
    "            data = pd.read_excel('C:/Users/Researcher/Desktop/GAN_newsimu/hpc_data/multi_accuracy/only full data_k='\n",
    "                                 + str(i) + 'misrate=' + str(j) + '.xlsx')\n",
    "            data = data.as_matrix()\n",
    "\n",
    "            # import data with missing value\n",
    "            xx = pd.read_excel('C:/Users/Researcher/Desktop/GAN_newsimu/hpc_data/multi_accuracy/only missing data_k='\n",
    "                               + str(i) + 'misrate=' + str(j) + '.xlsx')\n",
    "            xx = xx.as_matrix()\n",
    "\n",
    "            # about continuous and categorical\n",
    "            con = np.array(\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "            con = [con, ] * Train_No\n",
    "            con = np.asarray(con)\n",
    "\n",
    "            # set up mask\n",
    "            mask = np.isnan(xx)\n",
    "            mask = mask + 0\n",
    "            mask = 1. - mask\n",
    "\n",
    "            # normalization (mydata is to use)\n",
    "            xmax = np.nanmax(xx, axis=0)\n",
    "\n",
    "            xmin = np.nanmin(xx, axis=0)\n",
    "\n",
    "            max_min = xmax - xmin\n",
    "            max_min = max_min + 1e-8\n",
    "            mydata = ((data - xmin) / max_min) * con + (1 - con) * data\n",
    "            mydata = np.asarray(mydata)\n",
    "\n",
    "            # imputation value(Z)\n",
    "            meanvalue = np.nanmean(a=mydata, axis=0)\n",
    "            meanZ = [meanvalue, ] * Train_No\n",
    "            meanZ = np.asarray(meanZ)\n",
    "\n",
    "\n",
    "            ###DEFINE function\n",
    "\n",
    "            # Z start from noise\n",
    "            def sample_Z(m, n):\n",
    "                return np.random.uniform(0., 1, size=[m, n])\n",
    "\n",
    "\n",
    "            # for hint\n",
    "            def sample_M(m, n, p):\n",
    "                A = np.random.uniform(0., 1., size=[m, n])\n",
    "                B = A > p\n",
    "                C = 1. * B\n",
    "                return C\n",
    "\n",
    "\n",
    "            # for sampling mb\n",
    "            def sample_idx(m, n):\n",
    "                A = np.random.permutation(m)\n",
    "                idx = A[:n]\n",
    "                return idx\n",
    "\n",
    "\n",
    "            # for start value\n",
    "            def xavier_init(size):\n",
    "                in_dim = size[0]\n",
    "                xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "                return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "            # for draw a picture\n",
    "            def plot(samples):\n",
    "                fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "                gs = gridspec.GridSpec(5, 5)\n",
    "\n",
    "                gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "                for i, sample in enumerate(samples):\n",
    "                    ax = plt.subplot(gs[i])\n",
    "\n",
    "                    plt.axis('off')\n",
    "\n",
    "                    ax.set_xticklabels([])\n",
    "\n",
    "                    ax.set_yticklabels([])\n",
    "\n",
    "                    ax.set_aspect('equal')\n",
    "\n",
    "                    plt.imshow(sample.reshape(5, 10), cmap='Greys_r')\n",
    "\n",
    "                return fig\n",
    "\n",
    "\n",
    "            ##define write to excel in for circle\n",
    "            '''\n",
    "            def write_excel_xls_append(path, value):\n",
    "                index = len(value)  # get the No of lines of the data to be write\n",
    "                workbook = xlrd.open_workbook(path)  # open the file\n",
    "                sheets = workbook.sheet_names()  # get all the sheets\n",
    "                worksheet = workbook.sheet_by_name(sheets[0])  # get the first sheet\n",
    "                rows_old = worksheet.nrows  # get the lines No already in the sheet\n",
    "                new_workbook = copy(workbook)  # transfer xlrd to xlwt\n",
    "                new_worksheet = new_workbook.get_sheet(0)  # get the first sheet of transfered sheet\n",
    "                for i in range(0, index):\n",
    "                    for j in range(0, len(value[i])):\n",
    "                        new_worksheet.write(i + rows_old, j, value[i][j])  # write from No i+rows_old line\n",
    "                new_workbook.save(path)\n",
    "                print(\"write to excel done！\")\n",
    "            '''\n",
    "\n",
    "\n",
    "            ##define batch normolization\n",
    "            def dense(x, size, scope):\n",
    "                return tf.contrib.layers.fully_connected(x, size, activation_fn=None, scope=scope)\n",
    "\n",
    "\n",
    "            def dense_batch_relu(x, size, phase, scope):\n",
    "                with tf.variable_scope(scope):\n",
    "                    h1 = tf.contrib.layers.fully_connected(x, size, activation_fn=None, scope='dense')\n",
    "                    h2 = tf.contrib.layers.batch_norm(h1, center=True, scale=True, is_training=phase, scope='bn')\n",
    "                    return tf.nn.tanh(h2, 'relu')\n",
    "\n",
    "\n",
    "            ###BUILD STRUCTURE\n",
    "            # placeholder\n",
    "\n",
    "            with tf.name_scope('input_layer'):\n",
    "                X = tf.placeholder(tf.float32, shape=[None, Dim], name='X_input')  # data vector with missing value\n",
    "                M = tf.placeholder(tf.float32, shape=[None, Dim], name='Y')  # mask vector\n",
    "                H = tf.placeholder(tf.float32, shape=[None, Dim], name='H')  # hint vector\n",
    "                Z = tf.placeholder(tf.float32, shape=[None, Dim], name='Z')  # mean vector\n",
    "                CO = tf.placeholder(tf.float32, shape=[None, Dim], name='CO')  # continuous variables vector\n",
    "                phase1 = tf.placeholder(tf.bool, name='phase1')\n",
    "                phase2 = tf.placeholder(tf.bool, name='phase2')\n",
    "            # parameters to be calculate by the GAIN(theta_D, theta_G)\n",
    "            '''\n",
    "            D_W1 = tf.Variable(xavier_init([Dim * 2, n_hidden1]))  # Data and Hint are inputs\n",
    "            D_b1 = tf.Variable(tf.zeros(shape=[n_hidden1]))\n",
    "            D_W2 = tf.Variable(xavier_init([n_hidden1, n_hidden2]))\n",
    "            D_b2 = tf.Variable(tf.zeros(shape=[n_hidden2]))\n",
    "            D_W3 = tf.Variable(xavier_init([n_hidden2, Dim]))\n",
    "            D_b3 = tf.Variable(tf.zeros(shape=[Dim]))\n",
    "            theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "            G_W1 = tf.Variable(xavier_init([Dim * 2, n_hidden1]))  # Data and hint as inputs (mean value are in Missing Components)\n",
    "            G_b1 = tf.Variable(tf.zeros(shape=[n_hidden1]))\n",
    "            G_W2 = tf.Variable(xavier_init([n_hidden1, n_hidden2]))\n",
    "            G_b2 = tf.Variable(tf.zeros(shape=[n_hidden2]))\n",
    "            G_W3 = tf.Variable(xavier_init([n_hidden2, Dim]))\n",
    "            G_b3 = tf.Variable(tf.zeros(shape=[Dim]))\n",
    "            theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
    "            '''\n",
    "\n",
    "\n",
    "            # Define generator\n",
    "            def generator(x, z, m):\n",
    "                inp = x * m + z * (1 - m)\n",
    "                input = tf.concat(axis=1, values=[inp, m])\n",
    "                # G_hidden1_1= tf.layers.dense(input, n_hidden1, name=\"G_hidden1\")\n",
    "                # G_bn1 = tf.layers.batch_normalization(G_hidden1_1, training=training1, momentum=0.9)\n",
    "                # G_hidden1 = tf.nn.elu(G_bn1)\n",
    "\n",
    "                # G_hidden2_1 = tf.layers.dense(G_hidden1, n_hidden2, name=\"G_hidden2\")\n",
    "                # G_bn2 = tf.layers.batch_normalization(G_hidden2_1, training=training1, momentum=0.9)\n",
    "                # G_hidden2 = tf.nn.elu(G_bn2)\n",
    "\n",
    "                # G_logits_before_bn = tf.layers.dense(G_hidden2, Dim, name=\"G_outputs\"\n",
    "                # G_logits = tf.layers.batch_normalization(G_logits_before_bn, training=training1,momentum=0.9)\n",
    "                G_h1 = dense_batch_relu(input, n_hidden1, phase1, 'Glayer1')\n",
    "                G_h2 = dense_batch_relu(G_h1, n_hidden2, phase1, 'Glayer2')\n",
    "                G_logits = dense(x=G_h2, size=Dim, scope='G_logits')\n",
    "\n",
    "                G_prob = tf.nn.sigmoid(G_logits)\n",
    "                return G_prob\n",
    "\n",
    "\n",
    "            def discriminator(x, m, g, h):\n",
    "                inp = x * m + g * (1 - m)\n",
    "                input = tf.concat(axis=1, values=[inp, h])\n",
    "                # D_hidden1_1 = tf.layers.dense(input, n_hidden1, name=\"D_hidden1\")\n",
    "                # D_bn1 = tf.layers.batch_normalization(D_hidden1_1, training=training2, momentum=0.9)\n",
    "                # D_hidden1 = tf.nn.elu(D_bn1)\n",
    "\n",
    "                # D_hidden2_1 = tf.layers.dense(D_hidden1, n_hidden2, name=\"D_hidden2\")\n",
    "                # D_bn2 = tf.layers.batch_normalization(D_hidden2_1, training=training2, momentum=0.9)\n",
    "                # D_hidden2 = tf.nn.elu(D_bn2)\n",
    "\n",
    "                # D_logits_before_bn = tf.layers.dense(D_hidden2, Dim, name=\"D_outputs\")\n",
    "                # D_logit = tf.layers.batch_normalization(D_logits_before_bn, training=training2,momentum=0.9)\n",
    "\n",
    "                D_h1 = dense_batch_relu(input, n_hidden1, phase2, 'Dlayer1')\n",
    "                D_h2 = dense_batch_relu(D_h1, n_hidden2, phase2, 'D_layer2')\n",
    "                D_logits = dense(x=D_h2, size=Dim, scope='D_s_logits')\n",
    "                D_prob = tf.nn.sigmoid(D_logits)\n",
    "                return D_prob\n",
    "\n",
    "\n",
    "            # model constructure\n",
    "            G_sample = generator(X, Z, M)\n",
    "            D_result = discriminator(X, M, G_sample, H)\n",
    "\n",
    "            # define loss\n",
    "            with tf.name_scope('D_loss'):\n",
    "                D_loss = -tf.reduce_mean(M * tf.log(tf.clip_by_value(D_result, 1e-8, 1.)) + (1 - M) * tf.log(\n",
    "                    tf.clip_by_value(1. - D_result, 1e-8, 1.))) * 2  ##D_loss is sigmoid cross-entropy to tell true/fake\n",
    "                tf.summary.scalar('D_loss', D_loss)\n",
    "            with tf.name_scope('G_loss1'):\n",
    "                G_loss1 = -tf.reduce_mean((1 - M) * tf.log(tf.clip_by_value(D_result, 1e-8, 1.))) / tf.reduce_mean(\n",
    "                    1 - M)  ##G_loss1 to min the possibility that D tell the true/fake\n",
    "                tf.summary.scalar('G_loss1', G_loss1)\n",
    "            with tf.name_scope('MSE_train'):\n",
    "                MSE_train = tf.reduce_mean((M * X * CO - M * G_sample * CO) ** 2) / tf.reduce_mean(M * CO)\n",
    "                tf.summary.scalar('MSE_train', MSE_train)\n",
    "            with tf.name_scope('CROSS_train'):\n",
    "                CROSS_train = -tf.reduce_mean(\n",
    "                    (1 - CO) * X * M * tf.log(tf.clip_by_value(G_sample, 1e-8, 1)) + (1 - X) * (1 - CO) * M * tf.log(\n",
    "                        tf.clip_by_value(1 - G_sample, 1e-8, 1.)))\n",
    "                tf.summary.scalar('CROSS_train', CROSS_train)\n",
    "            with tf.name_scope('G_loss'):\n",
    "                G_loss = gamma * G_loss1 + alpha * MSE_train + beta * CROSS_train\n",
    "                tf.summary.scalar('G_loss', G_loss)\n",
    "\n",
    "            # test performance\n",
    "            MSE_test = tf.reduce_mean(((1 - M) * X * CO - (1 - M) * G_sample * CO) ** 2) / tf.reduce_mean((1 - M) * CO)\n",
    "            CROSS_test = -tf.reduce_mean(\n",
    "                (1 - CO) * X * (1 - M) * tf.log(tf.clip_by_value(G_sample, 1e-8, 1.)) + (1 - X) * (1 - CO) * (\n",
    "                        1 - M) * tf.log(\n",
    "                    tf.clip_by_value(1 - G_sample, 1e-8, 1.)))\n",
    "\n",
    "            # solver\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(extra_update_ops):\n",
    "                D_solver = tf.train.GradientDescentOptimizer(0.2).minimize(D_loss)\n",
    "            with tf.control_dependencies(extra_update_ops):\n",
    "                G_solver = tf.train.GradientDescentOptimizer(0.5).minimize(G_loss)\n",
    "\n",
    "            ### RUN\n",
    "            sess = tf.Session()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            merged = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "            ###for calculation moving_mean and moving_variance to calculate BN\n",
    "            '''\n",
    "            if not os.path.exists('Dovey_exp_out1/'):\n",
    "                os.makedirs('Dovey_exp_out1/')\n",
    "\n",
    "            i = 1\n",
    "            '''\n",
    "            for it in tqdm(range(interations)):\n",
    "                mb_idx = sample_idx(Train_No, mb_size)\n",
    "                X_mb = mydata[mb_idx, :]\n",
    "                M_mb = mask[mb_idx, :]\n",
    "                Z_mb = meanZ[mb_idx, :]\n",
    "                # Z_mb = sample_Z(mb_size, Dim)\n",
    "                H_mb1 = sample_M(mb_size, Dim, 1 - p_hint)\n",
    "                H_mb = M_mb * H_mb1\n",
    "                CO_mb = con[mb_idx, :]\n",
    "                startX = M_mb * X_mb + (1 - M_mb) * Z_mb\n",
    "\n",
    "                ##final\n",
    "                finalX = mask * mydata + (1 - mask) * meanZ\n",
    "                H_final1 = sample_M(Train_No, Dim, 1 - p_hint)\n",
    "                H_final = H_final1 * mask\n",
    "                ###\n",
    "                _, D_loss_curr = sess.run([D_solver, D_loss],\n",
    "                                          feed_dict={X: X_mb, M: M_mb, Z: startX, H: H_mb, CO: CO_mb, phase1: 1,\n",
    "                                                     phase2: 1})\n",
    "                _, G_loss_curr, MSE_train_curr, CROSS_train_curr = sess.run(\n",
    "                    [G_solver, G_loss1, MSE_train, CROSS_train],\n",
    "                    feed_dict={X: X_mb, M: M_mb, Z: startX, H: H_mb, CO: CO_mb, phase1: 1, phase2: 1})\n",
    "                MSE_test_curr, CROSS_test_curr = sess.run([MSE_test, CROSS_test],\n",
    "                                                          feed_dict={X: X_mb, M: M_mb, Z: startX, H: H_mb, CO: CO_mb,\n",
    "                                                                     phase1: 0, phase2: 0})\n",
    "                MSE_final, CROSS_final = sess.run([MSE_test, CROSS_test],\n",
    "                                                  feed_dict={X: mydata, M: mask, Z: finalX, CO: con, phase1: 0,\n",
    "                                                             phase2: 0})\n",
    "                '''if it % 500 == 0:\n",
    "                    mb_idx = sample_idx(Train_No, 5)\n",
    "                    X_mb = mydata[mb_idx, :]\n",
    "                    M_mb = mask[mb_idx, :]\n",
    "                    Z_mb = meanZ[mb_idx, :]\n",
    "                    startX = M_mb * X_mb + (1 - M_mb) * Z_mb\n",
    "                    samples1 = X_mb\n",
    "                    samples2 = M_mb * X_mb + (1 - M_mb) * Z_mb\n",
    "                    samples31 = sess.run(G_sample, feed_dict={X: X_mb, M: M_mb, Z: startX})\n",
    "                    samples3 = M_mb * X_mb + (1 - M_mb) * samples31\n",
    "                    samples = np.vstack([samples1, samples2, samples3])\n",
    "                    fig = plot(samples)\n",
    "                    plt.savefig('Dovey_exp_out1/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "                    i += 1\n",
    "                    plt.close(fig)\n",
    "                '''\n",
    "                if it % 3000 == 0:\n",
    "                    print('Iteraction:{}'.format(it))\n",
    "                    print('G_loss:{:.4}'.format(G_loss_curr))\n",
    "                    print('D_loss:{:.4}'.format(D_loss_curr))\n",
    "                    print('Train_MSE_loss: {:.4}'.format(MSE_train_curr))\n",
    "                    print('Train_CROSS_loss: {:.4}'.format(CROSS_train_curr))\n",
    "\n",
    "                    print('Test_MSE_loss: {:.4}'.format(MSE_test_curr))\n",
    "                    print('Test_CROSS_loss: {:.4}'.format(CROSS_test_curr))\n",
    "\n",
    "                    print('Final_MSE: {:.4}'.format(MSE_final))\n",
    "                    print('Final_CROSS: {:.4}'.format(CROSS_final))\n",
    "                    print()\n",
    "\n",
    "                    # it_No = it\n",
    "                    # output = np.array([[it_No, G_loss_curr, D_loss_curr,\n",
    "                    # MSE_train_curr, CROSS_train_curr, MSE_test_curr, CROSS_test_curr, MSE_final, CROSS_final], ])\n",
    "                    # excel_name = 'Hyper_para_search_new_March13.xls'\n",
    "                    # write_excel_xls_append(excel_name, output)\n",
    "\n",
    "                if it % 3000 == 0:\n",
    "                    X_final = mydata\n",
    "                    M_final = mask\n",
    "                    Z_final = meanZ\n",
    "                    CO_final = con\n",
    "                    finalX = M_final * X_final + (1 - M_final) * Z_final\n",
    "                    sample_final1 = sess.run(G_sample,\n",
    "                                             feed_dict={X: mydata, M: mask, Z: finalX, CO: con, phase1: 0, phase2: 0})\n",
    "                    sample_final2 = X_final * M_final + (1 - M_final) * sample_final1\n",
    "                    sample_final = sample_final2 * max_min + xmin\n",
    "                    sample_final[:, 15:22] = np.round(sample_final[:, 15:22], 0)\n",
    "                    ##export\n",
    "                    writer = pd.ExcelWriter('C:/Users/Researcher/Desktop/GAN_newsimu/hpc_data/multi_accuracy/gain_k='\n",
    "                                            + str(i) + 'misrate=' + str(j) + '_i='+str(q)+'.xlsx')\n",
    "                    sample_final_write = pd.DataFrame(sample_final)\n",
    "                    # sample_final_write = sample_final_write.round({'15':0,'16':0,'17':0,'18':0,'19':0,'20':0})\n",
    "                    sample_final_write.to_excel(writer, 'Sheet1')\n",
    "                    writer.save()\n",
    "                '''\n",
    "                if it % 15000 ==0:\n",
    "                    it_No = it\n",
    "                    output = np.array([[mb_size,p_hint,gamma, alpha,beta, it_No,G_loss_curr, D_loss_curr,MSE_train_curr,\n",
    "                                        CROSS_train_curr, MSE_test_curr,CROSS_test_curr, MSE_final, CROSS_final],])\n",
    "                    excel_name = 'Hyper_para_search_weights.xls'\n",
    "                    write_excel_xls_append(excel_name, output)\n",
    "                '''\n",
    "            print('Final_MSE: {:.4}'.format(MSE_final))\n",
    "            print('Final_CROSS: {:.4}'.format(CROSS_final))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
