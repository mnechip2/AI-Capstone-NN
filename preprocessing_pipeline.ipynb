{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a94fbb",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocessing Pipeline\n",
    "\n",
    "## NeuroSense Analytics \n",
    "\n",
    "### v0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42378d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77eacc",
   "metadata": {},
   "source": [
    "Creating a `preprocessor` class to automate the import workflow - preprocessor handles data import, datatype casting, one-hot encoding, MICE imputation, joining feature data with survey data and returns a Polars dataframe or NumPy series.  \n",
    "\n",
    "Output is in the shape of `batch_size`, `time_steps`, `features` for LSTM processing.  \n",
    "\n",
    "## DataPreprocessor  \n",
    "\n",
    "> ### Parameters:  \n",
    "\n",
    "- `path`: *`int`*  \n",
    "Data path to use (INS_W1 = `1`, INS_W2 = `2`, INS_W3 = `3`, INS_W4 = `4`)  \n",
    "\n",
    "- `imputer_max_iter`: *`int`, default = `10`*  \n",
    "Max amount of iterations for IterativeImputer.  \n",
    "\n",
    "- `imputer_random_state`: *`int`, default = `42`*  \n",
    "Imputer random state.  \n",
    "\n",
    "- `nearest_features`: *`int`, default = `None`*  \n",
    "How many neighbours to sample when imputing.  \n",
    "\n",
    "- `strategy`: *`{‘mean’, ‘median’, ‘most_frequent’, ‘constant’}`, default = `median`*  \n",
    "What strategy to use when imputing  \n",
    "\n",
    "- `impute`: *`bool`, default = `True`*  \n",
    "Whether or not to run imputation.  \n",
    "\n",
    "- `exclude_history`: *`bool`, default = `True`*  \n",
    "Whether or not to exclude 14- and 7-day histories during preprocessing.\n",
    "\n",
    "> ### Functions:  \n",
    "\n",
    "- `import_csv_feature_data`:  \n",
    "\n",
    "    - `csv`: *`str`*   \n",
    "    File name to be preprocessed  \n",
    "\n",
    "> ### Example Usage:   \n",
    "\n",
    "`preprocessor_INS_W1 = DataPreprocessor(DATA_PATH_1, imputer_max_iter=20,impute=True)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "31a9029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script imports CSV files containing feature and survey data, processes them, and prepares them for analysis.\n",
    "# It also includes functions to load and preprocess the data, including scaling and encoding categorical variables.\n",
    "\n",
    "# data path for the CSV files\n",
    "# The data is organized into four directories, each containing feature and survey data.\n",
    "DATA_PATH_1 = \"./csv_data/INS-W_1/\"\n",
    "DATA_PATH_2 = \"./csv_data/INS-W_2/\"\n",
    "DATA_PATH_3 = \"./csv_data/INS-W_3/\"\n",
    "DATA_PATH_4 = \"./csv_data/INS-W_4/\"\n",
    "\n",
    "# setting up scaler for MinMax scaling\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, path: str, imputer_max_iter: int = 10, imputer_random_state: int = 42, nearest_features: int = None, strategy: str = \"mean\", exclude_history: bool = True, impute: bool = True): # Initialize the DataPreprocessor class\n",
    "        self.path = path\n",
    "        self.exclude_history = exclude_history # Exclude history data if specified\n",
    "        self.scaler = MinMaxScaler() # Initialize the MinMaxScaler with specified parameters\n",
    "        self.impute = impute\n",
    "        self.imputer = IterativeImputer(max_iter=imputer_max_iter, random_state=imputer_random_state, n_nearest_features=nearest_features, initial_strategy=strategy) # Initialize the IterativeImputer with specified parameters\n",
    "\n",
    "    # Load the CSV files, cast columns to appropriate types, and drop empty columns\n",
    "    def import_csv_feature_data(self, file_name: str) -> pl.DataFrame:\n",
    "        try:\n",
    "            if self.exclude_history:\n",
    "                q = (\n",
    "                    pl.scan_csv(self.path + \"FeatureData/\" + file_name + \".csv\")\n",
    "                    .select(pl.col(\"*\"))\n",
    "                    .cast({\"date\": pl.Date})\n",
    "                    .drop(\"\")\n",
    "                    .with_columns(pl.col(\"pid\").str.replace_all(\"INS-W_\",\"\"))\n",
    "                    .cast({\"pid\": pl.Int32})\n",
    "                    .select(pl.exclude(pl.String))\n",
    "                    .select(pl.exclude([\"^.*14dhist$\", \"^.*7dhist$\"]))\n",
    "                )\n",
    "            else:\n",
    "                q = (\n",
    "                    pl.scan_csv(self.path + \"FeatureData/\" + file_name + \".csv\")\n",
    "                    .select(pl.col(\"*\"))\n",
    "                    .cast({\"date\": pl.Date})\n",
    "                    .drop(\"\")\n",
    "                    .with_columns(pl.col(\"pid\").str.replace_all(\"INS-W_\",\"\"))\n",
    "                    .cast({\"pid\": pl.Int32})\n",
    "                    .select(pl.exclude(pl.String))\n",
    "                )\n",
    "            data = q.collect() # Collect the lazy frame into a DataFrame\n",
    "\n",
    "            if self.impute:\n",
    "                scaled_data = pl.from_numpy( # Convert to numpy array for scaling\n",
    "                        self.scaler.fit_transform(data.select(pl.exclude([pl.Date, pl.Int32]))), schema=data.select(pl.exclude([pl.Date, pl.Int32])).columns # min max scaling on all columns except date and pid\n",
    "                    )\n",
    "                try:\n",
    "                    self.imputer.fit(scaled_data) # Fit the imputer to the scaled data\n",
    "                    imputed_data = pl.from_numpy(\n",
    "                        self.imputer.transform(scaled_data), schema=data.select(pl.exclude([\"pid\",\"date\"])).columns # Transform the scaled data using the imputer\n",
    "                    )\n",
    "                    data = data.select([\"pid\",\"date\"])\n",
    "                    data = data.hstack(imputed_data) # Add imputed data back to the DataFrame\n",
    "                    del imputed_data # Delete the imputed data variable to free up memory\n",
    "                except: \n",
    "                    print(\"Error in imputation, returning scaled data without imputation.\")\n",
    "                    return data\n",
    "                return data\n",
    "            return data\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error importing feature data from {self.path + 'FeatureData/' + file_name}: {e}\")\n",
    "            return pl.DataFrame()\n",
    "\n",
    "    def import_csv_survey_data(self, file_name: str) -> pl.DataFrame:\n",
    "        try: # Load survey data from CSV file\n",
    "            q = (\n",
    "                pl.scan_csv(self.path + \"SurveyData/\" + file_name + \".csv\")\n",
    "                .select(pl.col(\"*\"))\n",
    "                .cast({\"date\": pl.Date})\n",
    "                .drop(\"\")\n",
    "                .with_columns(pl.col(\"pid\").str.replace_all(\"INS-W_\",\"\"))\n",
    "                .cast({\"pid\": pl.Int32})\n",
    "            )\n",
    "            data = q.collect()\n",
    "            match file_name:\n",
    "                case \"ema\":\n",
    "                    survey_data = data.select(pl.exclude([\"pid\",\"date\"])) # Convert to numpy array for scaling\n",
    "                    scaled_data = pl.from_numpy(\n",
    "                        self.scaler.fit_transform(survey_data), schema=survey_data.columns\n",
    "                    )\n",
    "                    data = data.select([\"pid\",\"date\"])\n",
    "                    data = data.hstack(scaled_data)\n",
    "                    del scaled_data\n",
    "                    return data\n",
    "                case \"post\":\n",
    "                    survey_data = data.select(pl.exclude([\"pid\",\"date\"])) # Convert to numpy array for scaling\n",
    "                    scaled_data = pl.from_numpy(\n",
    "                        self.scaler.fit_transform(survey_data), schema=survey_data.columns\n",
    "                    )\n",
    "                    data = data.select([\"pid\",\"date\"])\n",
    "                    data = data.hstack(scaled_data)\n",
    "                    del scaled_data\n",
    "                    return data\n",
    "                case \"pre\":\n",
    "                    survey_data = data.select(pl.exclude([\"pid\",\"date\"]))\n",
    "                    scaled_data = pl.from_numpy(\n",
    "                        self.scaler.fit_transform(survey_data), schema=survey_data.columns\n",
    "                    )\n",
    "                    data = data.select([\"pid\",\"date\"])\n",
    "                    data = data.hstack(scaled_data)\n",
    "                    del scaled_data\n",
    "                    return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error importing survey data from {self.path + 'SurveyData/' + file_name}: {e}\")\n",
    "            return pl.DataFrame()\n",
    "\n",
    "    def import_dep_endterm(self) -> pl.DataFrame:\n",
    "        try:\n",
    "            q = (\n",
    "                pl.scan_csv(self.path + \"SurveyData/dep_endterm.csv\")\n",
    "                .select(pl.col(\"*\"))\n",
    "                .cast({\"date\": pl.Date})\n",
    "                .drop(\"\")\n",
    "                .with_columns(pl.col(\"pid\").str.replace_all(\"INS-W_\",\"\"))\n",
    "                .cast({\"pid\": pl.Int32})    \n",
    "                )\n",
    "            data = q.collect()\n",
    "            bdi2 = data.select(pl.exclude([\"pid\",\"date\", \"dep\"]))\n",
    "            data_scaled = pl.from_numpy(\n",
    "                self.scaler.fit_transform(bdi2), schema=bdi2.columns # min max scaling on all columns except date and pid\n",
    "            )\n",
    "            data = data.select([\"pid\",\"date\", \"dep\"])\n",
    "            data = data.hstack(data_scaled) # Add scaled data back to the DataFrame\n",
    "            del data_scaled # Delete the scaled data variable to free up memory\n",
    "            data = data.to_dummies(\"dep\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error importing endterm data from {self.path + 'SurveyData/dep_endterm.csv'}: {e}\")\n",
    "            return pl.DataFrame()\n",
    "\n",
    "    def merge_survey_to_feature(self, feature_data: pl.DataFrame, survey_data: pl.DataFrame) -> pl.DataFrame:\n",
    "        try:\n",
    "            merged_data = feature_data.join(survey_data, on=[\"pid\"], how=\"inner\")\n",
    "            return merged_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging feature and survey data: {e}\")\n",
    "            return pl.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425fef18",
   "metadata": {},
   "source": [
    "# INS-W_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3eb2e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_INS_W1 = DataPreprocessor(DATA_PATH_1, imputer_max_iter=20,impute=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
