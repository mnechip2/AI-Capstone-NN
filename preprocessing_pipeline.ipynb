{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a94fbb",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocessing Pipeline\n",
    "\n",
    "## NeuroSense Analytics \n",
    "\n",
    "### v0.0.1 - 17/04/2025\n",
    "\n",
    "Project description placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42378d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77eacc",
   "metadata": {},
   "source": [
    "Creating a `preprocessor` class to automate the import workflow - preprocessor handles data import, datatype casting, one-hot encoding, MICE imputation, joining feature data with survey data and returns a Polars dataframe or NumPy series.  \n",
    "\n",
    "Output is in the shape of `batch_size`, `time_steps`, `features` for LSTM processing.  \n",
    "\n",
    "Functions:  \n",
    "- `import_csv_feature_data`: \n",
    "    - `csv`: `String` of file name to be preprocessed\n",
    "\n",
    "Accepted arguments are:\n",
    "- `path`: `Int`, data path to use (INS_W1 = `1`, INS_W2 = `2`, INS_W3 = `3`, INS_W4 = `4`)\n",
    "- `imputer_max_iter`: `Int`, max amount of iterations for IterativeImputer\n",
    "- `imputer_random_state`: `Int`, imputer random state\n",
    "- `impute`: `bool`, whether or not to run imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a9029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script imports CSV files containing feature and survey data, processes them, and prepares them for analysis.\n",
    "# It also includes functions to load and preprocess the data, including scaling and encoding categorical variables.\n",
    "\n",
    "# data path for the CSV files\n",
    "# The data is organized into four directories, each containing feature and survey data.\n",
    "DATA_PATH_1 = \"./csv_data/INS-W_1/\"\n",
    "DATA_PATH_2 = \"./csv_data/INS-W_2/\"\n",
    "DATA_PATH_3 = \"./csv_data/INS-W_3/\"\n",
    "DATA_PATH_4 = \"./csv_data/INS-W_4/\"\n",
    "\n",
    "# setting up scaler for MinMax scaling\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, path: str, imputer_max_iter: int, imputer_random_state: int, impute: bool = False): # Initialize the DataPreprocessor class\n",
    "        self.path = path\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.impute = impute\n",
    "        self.imputer = IterativeImputer(max_iter=imputer_max_iter, random_state=imputer_random_state) # Initialize the IterativeImputer with specified parameters\n",
    "\n",
    "    # Load the CSV files, cast columns to appropriate types, and drop empty columns\n",
    "    def import_csv_feature_data(self, file_name: str) -> pl.DataFrame:\n",
    "        try:\n",
    "            q = (\n",
    "                pl.scan_csv(self.path + \"FeatureData/\" + file_name + \".csv\")\n",
    "                .select(pl.col(\"*\"))\n",
    "                .cast({\"date\": pl.Date})\n",
    "                .drop(\"\")\n",
    "                .with_columns(pl.col(\"pid\").str.replace_all(\"INS-W_\",\"\"))\n",
    "                .cast({\"pid\": pl.Int32})\n",
    "                .select(pl.exclude(pl.String))\n",
    "            )\n",
    "            data = q.collect() # Collect the lazy frame into a DataFrame\n",
    "\n",
    "            if self.impute:\n",
    "                scaled_data = pl.from_numpy( # Convert to numpy array for scaling\n",
    "                        self.scaler.fit_transform(data.select(pl.exclude([pl.Date, pl.Int32]))), schema=data.select(pl.exclude([pl.Date, pl.Int32])).columns # min max scaling on all columns except date and pid\n",
    "                    )\n",
    "                try:\n",
    "                    self.imputer.fit(scaled_data) # Fit the imputer to the scaled data\n",
    "                    imputed_data = pl.from_numpy(\n",
    "                        self.imputer.transform(scaled_data), schema=data.select(pl.exclude([\"pid\",\"date\"])).columns # Transform the scaled data using the imputer\n",
    "                    )\n",
    "                    data = data.select([\"pid\",\"date\"])\n",
    "                    data = data.hstack(imputed_data) # Add imputed data back to the DataFrame\n",
    "                    del imputed_data # Delete the imputed data variable to free up memory\n",
    "                except: \n",
    "                    print(\"Error in imputation, returning scaled data without imputation.\")\n",
    "                    return data\n",
    "                return data\n",
    "            return data\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error importing feature data from {self.path + 'FeatureData/' + file_name}: {e}\")\n",
    "            return pl.DataFrame()\n",
    "\n",
    "    def import_csv_survey_data(self, file_name: str) -> pl.DataFrame:\n",
    "        try: # Load survey data from CSV file\n",
    "            q = (\n",
    "                pl.scan_csv(self.path + \"SurveyData/\" + file_name + \".csv\")\n",
    "                .select(pl.col(\"*\"))\n",
    "                .cast({\"date\": pl.Date})\n",
    "                .drop(\"\")\n",
    "                .with_columns(pl.col(\"pid\").str.replace_all(\"INS-W_\",\"\"))\n",
    "                .cast({\"pid\": pl.Int32})\n",
    "            )\n",
    "            data = q.collect()\n",
    "            match file_name:\n",
    "                case \"ema\":\n",
    "                    survey_data = data.select(pl.exclude([\"pid\",\"date\"])) # Convert to numpy array for scaling\n",
    "                    scaled_data = pl.from_numpy(\n",
    "                        self.scaler.fit_transform(survey_data), schema=survey_data.columns\n",
    "                    )\n",
    "                    data = data.select([\"pid\",\"date\"])\n",
    "                    data = data.hstack(scaled_data)\n",
    "                    del scaled_data\n",
    "                    return data\n",
    "                case \"post\":\n",
    "                    survey_data = data.select(pl.exclude([\"pid\",\"date\"])) # Convert to numpy array for scaling\n",
    "                    scaled_data = pl.from_numpy(\n",
    "                        self.scaler.fit_transform(survey_data), schema=survey_data.columns\n",
    "                    )\n",
    "                    data = data.select([\"pid\",\"date\"])\n",
    "                    data = data.hstack(scaled_data)\n",
    "                    del scaled_data\n",
    "                    return data\n",
    "                case \"pre\":\n",
    "                    survey_data = data.select(pl.exclude([\"pid\",\"date\"]))\n",
    "                    scaled_data = pl.from_numpy(\n",
    "                        self.scaler.fit_transform(survey_data), schema=survey_data.columns\n",
    "                    )\n",
    "                    data = data.select([\"pid\",\"date\"])\n",
    "                    data = data.hstack(scaled_data)\n",
    "                    del scaled_data\n",
    "                    return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error importing survey data from {self.path + 'SurveyData/' + file_name}: {e}\")\n",
    "            return pl.DataFrame()\n",
    "\n",
    "    def import_csv_dep_data_endterm(self) -> pl.DataFrame:\n",
    "        try:\n",
    "            q = (\n",
    "                pl.scan_csv(self.path + \"SurveyData/dep_endterm.csv\")\n",
    "                .select(pl.col(\"*\"))\n",
    "                .cast({\"date\": pl.Date})\n",
    "                .drop(\"\")\n",
    "                .with_columns(pl.col(\"pid\").str.replace_all(\"INS-W_\",\"\"))\n",
    "                .cast({\"pid\": pl.Int32})    \n",
    "                )\n",
    "            data = q.collect()\n",
    "            data = data.to_dummies(\"dep\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error importing endterm data from {self.path + 'SurveyData/dep_endterm.csv'}: {e}\")\n",
    "            return pl.DataFrame()\n",
    "\n",
    "    def merge_survey_to_feature(self, feature_data: pl.DataFrame, survey_data: pl.DataFrame) -> pl.DataFrame:\n",
    "        try:\n",
    "            merged_data = feature_data.join(survey_data, on=[\"pid\", \"date\"], how=\"inner\")\n",
    "            return merged_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging feature and survey data: {e}\")\n",
    "            return pl.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425fef18",
   "metadata": {},
   "source": [
    "# INS-W_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb2e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_INS_W1 = DataPreprocessor(DATA_PATH_1, imputer_max_iter=40, imputer_random_state=42, impute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde4911e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\darkenral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wifi_1 = preprocessor_INS_W1.import_csv_feature_data(\"wifi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
